{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-07 01:01:27.271226: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-07 01:01:27.271260: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "from summarizers import get_book_descriptions\n",
    "from summarizers import frequent_ngrams\n",
    "from summarizers import frequent_arbitrarygrams\n",
    "from summarizers import sentimental_words\n",
    "from summarizers import squish\n",
    "from batch_processor import get_reviews_text\n",
    "from batch_processor import token_desc_to_str\n",
    "from main import print_descriptions\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_SPOILERS = True\n",
    "ASPECTS = [\"book\", \"story\", \"writing\", \"plot\", \"character\", \"protagonist\", \"relationship\", \"dialogue\", \"action\", \"pacing\"]\n",
    "SENT_THRESHOLD = 0.6\n",
    "ENT_COUNT = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_id = \"3\"\n",
    "# harry potter and the sorcerer's stone = 3\n",
    "# twilight = 41865\n",
    "# going postal = 64222"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, title, series = get_reviews_text(book_id, FILTER_SPOILERS, \"fantasy_paranormal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [\n",
    "    lemmatizer.lemmatize(word, pos.lower()[0] if pos.lower()[0] in \"nvars\" else \"n\").lower()\n",
    "    for word, pos in nltk.pos_tag(tokenized)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({('harry', 'potter'): 3400, ('first', 'time'): 620, ('first', 'book'): 582, ('potter', 'series'): 416, ('potter', 'book'): 410, ('read', 'harry'): 277, ('year', 'old'): 242, ('year', 'ago'): 212, ('even', 'though'): 207, ('whole', 'series'): 203, ...})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequent_ngrams(lemmatized, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({('harry', 'potter', 'series'): 405, ('harry', 'potter', 'book'): 379, ('read', 'harry', 'potter'): 271, ('love', 'harry', 'potter'): 119, ('first', 'harry', 'potter'): 77, ('first', 'time', 'read'): 65, ('never', 'get', 'old'): 49, ('harry', 'potter', 'fan'): 46, ('like', 'harry', 'potter'): 37, ('11', 'year', 'old'): 37, ...})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequent_ngrams(lemmatized, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.89it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('harry', 'potter', 'and', 'the', 'sorcerer', 'stone'), 208),\n",
       " (('read', 'the', 'rest', 'of', 'the', 'series'), 82),\n",
       " (('to', 'read', 'the', 'rest', 'of', 'the'), 74),\n",
       " (('the', 'first', 'time', 'i', 'read'), 116),\n",
       " (('ca', 'wait', 'to', 'read', 'the'), 79),\n",
       " (('first', 'time', 'i', 'read', 'it'), 74),\n",
       " (('first', 'harry', 'potter', 'book'), 55),\n",
       " (('time', 'read', 'harry', 'potter'), 19),\n",
       " (('first', 'read', 'harry', 'potter'), 18),\n",
       " (('harry', 'potter', 'series'), 413),\n",
       " (('love', 'harry', 'potter'), 124),\n",
       " (('never', 'get', 'old'), 49),\n",
       " (('year', 'old'), 242),\n",
       " (('year', 'ago'), 212),\n",
       " (('even', 'though'), 207)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequent_arbitrarygrams(lemmatized, 2, 6, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:10<00:00,  2.14s/it]\n"
     ]
    }
   ],
   "source": [
    "arbitrarygrams = frequent_arbitrarygrams(lemmatized, 2, 6, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i', 'love', 'this', 'book', 'so', 'much') 23 6\n",
      "('one', 'of', 'the', 'best', 'book', 'i') 21 6\n",
      "('i', 'fell', 'in', 'love', 'with') 47 5\n",
      "('fell', 'in', 'love', 'with', 'the') 33 5\n",
      "('be', 'one', 'of', 'the', 'best') 32 5\n",
      "('i', 'love', 'the', 'harry', 'potter') 27 5\n",
      "('love', 'the', 'harry', 'potter', 'series') 25 5\n",
      "('perfect', 'perfect', 'perfect', 'perfect') 10 4\n",
      "('best', 'book', 'series', 'ever') 6 4\n",
      "('best', 'harry', 'potter', 'book') 5 4\n",
      "('love', 'rowling', 'write', 'style') 4 4\n",
      "('next', 'great', 'adventure') 22 3\n",
      "('love', 'every', 'minute') 12 3\n",
      "('perfectly', 'normal', 'thank') 11 3\n",
      "('great', 'book') 126 2\n",
      "('still', 'love') 91 2\n",
      "('absolutely', 'love') 78 2\n",
      "('great', 'read') 59 2\n"
     ]
    }
   ],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "for ambigram, freq in arbitrarygrams:\n",
    "    if sia.polarity_scores(\" \".join(ambigram))[\"compound\"] > 0.6:\n",
    "        print(ambigram, freq, len(ambigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('love', 3147, {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.6369}),\n",
       " ('great', 1121, {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.6249}),\n",
       " ('amazing', 637, {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.5859}),\n",
       " ('fun', 599, {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.5106}),\n",
       " ('best', 543, {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.6369}),\n",
       " ('wonderful', 389, {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.5719}),\n",
       " ('awesome', 261, {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.6249}),\n",
       " ('fantastic', 250, {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.5574}),\n",
       " ('loved', 236, {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.5994}),\n",
       " ('perfect', 232, {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.5719}),\n",
       " ('amaze', 230, {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.5423}),\n",
       " ('bad', 211, {'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.5423}),\n",
       " ('evil', 206, {'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.6597}),\n",
       " ('kind', 203, {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.5267}),\n",
       " ('happy', 171, {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.5719}),\n",
       " ('brilliant', 151, {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.5859}),\n",
       " ('kill', 132, {'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.6908}),\n",
       " ('hate', 127, {'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.5719}),\n",
       " ('excellent', 124, {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.5719}),\n",
       " ('beautiful', 124, {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.5994})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentimental_words(lemmatized, 20, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars 2763344\n",
      "words 510745\n"
     ]
    }
   ],
   "source": [
    "print(\"chars\", len(text))\n",
    "print(\"words\", len(text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_book_descriptions(book_id, text, ENT_COUNT, ASPECTS, SENT_THRESHOLD, title, series)\n",
    "names_mapping, character_descriptions, aspect_descriptions, sentimental_descriptions, doc = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_stopwords = {\n",
    "    \"character\", \"characters\", \"writer\", \"author\", \"one\", \"first\", \"second\", \"last\"\n",
    "    \"whole\", \"book\", \"books\", \"series\", \"man\", \"woman\", \"place\", \"next\",\n",
    "    \"able\", \"read\", \"reader\", \"main\", \"many\", \"previous\", \"1st\", \"2nd\", \"3rd\", \"4th\", \"6th\", \"7th\",\n",
    "    \"protagonist\", \"girl\", \"boy\", \"male\", \"female\", \"great\", \"hero\", \"heroine\",\n",
    "    \"year\", \"years\", \"old\", \"kid\", \"son\"\n",
    "}\n",
    "\n",
    "print_descriptions(character_descriptions, min_descs=10, extra_stopwords=extra_stopwords)\n",
    "print_descriptions(aspect_descriptions, min_descs=10, extra_stopwords=extra_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "forbidden_sentimentals = {\n",
    "    \"thing\", \"things\", \"one\", \"way\", \"part\", \"deal\", \"job\", \"first\",\n",
    "    \"book\", \"books\", \"serie\", \"series\", \"story\", \"stories\", \"read\",\n",
    "    \"literature\", \"sequel\"\n",
    "}\n",
    "print_descriptions(sentimental_descriptions, min_descs=10, extra_stopwords=forbidden_sentimentals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_desc_count = sum(len(word_descs[1]) for word_descs in character_descriptions.items())\n",
    "aspect_desc_count = sum(len(word_descs[1]) for word_descs in aspect_descriptions.items())\n",
    "sentimental_desc_count = sum(len(word_descs[1]) for word_descs in sentimental_descriptions.items())\n",
    "total = len(list(doc.sents))\n",
    "\n",
    "print(\"characters descriptions\", character_desc_count)\n",
    "print(\"aspects descriptions\", aspect_desc_count)\n",
    "print(\"sentimental descriptions\", sentimental_desc_count)\n",
    "print(\"total review sentences\", total)\n",
    "print()\n",
    "\n",
    "sum_ = character_desc_count + aspect_desc_count + sentimental_desc_count\n",
    "\n",
    "print(\"% of sentences yielding a description\", f\"{sum_ / total * 100:.2f} %\")\n",
    "print()\n",
    "\n",
    "count = 0\n",
    "unique_descs = 0\n",
    "word_descs = 0\n",
    "for descss in [character_descriptions, aspect_descriptions, sentimental_descriptions]:\n",
    "    for key, descs in descss.items():\n",
    "        count += len(descs)\n",
    "        unique_descs += len(set(token_desc_to_str(desc) for desc in descs))\n",
    "        word_descs += sum(len(desc) for desc in descs)\n",
    "\n",
    "print(\"total descriptions\", count)\n",
    "print(\"total unique descriptions\", unique_descs)\n",
    "print(\"total words in descriptsion\", word_descs)\n",
    "\n",
    "print(\"total words in reviews\", len(doc))\n",
    "\n",
    "print()\n",
    "print(\"% of words in descriptions\", f\"{word_descs / len(doc) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
