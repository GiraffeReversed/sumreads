{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 00:38:37.953810: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-06 00:38:37.953841: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "from summarizers import get_book_descriptions\n",
    "from summarizers import frequent_ngrams\n",
    "from summarizers import frequent_arbitrarygrams\n",
    "from summarizers import sentimental_words\n",
    "from batch_processor import get_reviews_text\n",
    "from main import print_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_SPOILERS = True\n",
    "ASPECTS = [\"book\", \"story\", \"writing\", \"characters\", \"pacing\"]\n",
    "SENT_THRESHOLD = 0.6\n",
    "ENT_COUNT = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_id = \"3\"\n",
    "# harry potter and the sorcerer's stone = 3\n",
    "# twilight = 41865\n",
    "# the expanse = 8855321\n",
    "# eric = 64218\n",
    "# guards guards = 64216"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, title, series = get_reviews_text(book_id, FILTER_SPOILERS, \"fantasy_paranormal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [\n",
    "    lemmatizer.lemmatize(word, pos.lower()[0] if pos.lower()[0] in \"nvars\" else \"n\").lower()\n",
    "    for word, pos in nltk.pos_tag(tokenized)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({('harry', 'potter'): 3400, ('first', 'time'): 620, ('first', 'book'): 582, ('potter', 'series'): 416, ('potter', 'book'): 410, ('read', 'harry'): 277, ('year', 'old'): 242, ('year', 'ago'): 212, ('even', 'though'): 207, ('whole', 'series'): 203, ...})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequent_ngrams(lemmatized, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({('harry', 'potter', 'series'): 405, ('harry', 'potter', 'book'): 379, ('read', 'harry', 'potter'): 271, ('love', 'harry', 'potter'): 119, ('first', 'harry', 'potter'): 77, ('first', 'time', 'read'): 65, ('never', 'get', 'old'): 49, ('harry', 'potter', 'fan'): 46, ('like', 'harry', 'potter'): 37, ('11', 'year', 'old'): 37, ...})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequent_ngrams(lemmatized, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('harry', 'potter', 'and', 'the', 'sorcerer', 'stone'), 208),\n",
       " (('read', 'the', 'rest', 'of', 'the', 'series'), 82),\n",
       " (('to', 'read', 'the', 'rest', 'of', 'the'), 74),\n",
       " (('the', 'first', 'time', 'i', 'read'), 116),\n",
       " (('ca', 'wait', 'to', 'read', 'the'), 79),\n",
       " (('first', 'time', 'i', 'read', 'it'), 74),\n",
       " (('first', 'harry', 'potter', 'book'), 55),\n",
       " (('time', 'read', 'harry', 'potter'), 19),\n",
       " (('first', 'read', 'harry', 'potter'), 18),\n",
       " (('harry', 'potter', 'series'), 413),\n",
       " (('love', 'harry', 'potter'), 124),\n",
       " (('never', 'get', 'old'), 49),\n",
       " (('year', 'old'), 242),\n",
       " (('year', 'ago'), 212),\n",
       " (('even', 'though'), 207)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequent_arbitrarygrams(lemmatized, 2, 6, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('love', 3147, {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.6369}),\n",
       " ('great', 1121, {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.6249}),\n",
       " ('amazing', 637, {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.5859}),\n",
       " ('fun', 599, {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.5106}),\n",
       " ('best', 543, {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.6369}),\n",
       " ('wonderful', 389, {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.5719}),\n",
       " ('awesome', 261, {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.6249}),\n",
       " ('fantastic', 250, {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.5574}),\n",
       " ('loved', 236, {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.5994}),\n",
       " ('perfect', 232, {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.5719}),\n",
       " ('amaze', 230, {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.5423}),\n",
       " ('bad', 211, {'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.5423}),\n",
       " ('evil', 206, {'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.6597}),\n",
       " ('kind', 203, {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.5267}),\n",
       " ('happy', 171, {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.5719}),\n",
       " ('brilliant', 151, {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.5859}),\n",
       " ('kill', 132, {'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.6908}),\n",
       " ('hate', 127, {'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.5719}),\n",
       " ('excellent', 124, {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.5719}),\n",
       " ('beautiful', 124, {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.5994})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentimental_words(lemmatized, 20, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars 2763344\n",
      "words 510745\n"
     ]
    }
   ],
   "source": [
    "print(\"chars\", len(text))\n",
    "print(\"words\", len(text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_book_descriptions(book_id, text, ENT_COUNT, ASPECTS, SENT_THRESHOLD, title, series)\n",
    "names_mapping, character_descriptions, aspect_descriptions, sentimental_descriptions, doc = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_color(word, *args, **kwargs):\n",
    "    sentiment = doc.vocab[word].sentiment\n",
    "\n",
    "    norm = mpl.colors.Normalize(vmin=-0.8, vmax=0.8)\n",
    "    cmap = cm.RdYlGn\n",
    "\n",
    "    m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "    color = tuple(map(lambda v: int(255*v), m.to_rgba(sentiment)[:3]))\n",
    "    return color if color != (254, 254, 189) else (200, 200, 160)\n",
    "\n",
    "# def print_descriptions(descs, min_descs=15, extra_stopwords=set()):\n",
    "#     forbidden = set(nltk.corpus.stopwords.words(\"english\")) | extra_stopwords | descs.keys()\n",
    "#     for key, descs in sorted(descs.items(), key=lambda pair: len(pair[1]), reverse=True):\n",
    "#         descs = [\" \".join(word.text if word.text != \"n't\" else \"not\" for word in desc).lower() for desc in descs]\n",
    "#         if len(descs) < min_descs:\n",
    "#             continue\n",
    "\n",
    "#         print(key, len(descs), end=\" \")\n",
    "\n",
    "#         from wordcloud import WordCloud\n",
    "#         import matplotlib.pyplot as plt\n",
    "    \n",
    "#         wordcloud = WordCloud(\n",
    "#             width=1200,\n",
    "#             height=600,\n",
    "#             stopwords=forbidden,\n",
    "#             collocation_threshold=10,\n",
    "#             color_func=word_to_color\n",
    "#         ).generate(\" \".join(descs))\n",
    "#         plt.imshow(wordcloud, interpolation='bilinear')\n",
    "#         plt.axis(\"off\")\n",
    "#         plt.show()\n",
    "        \n",
    "#         print(*sorted(set(descs), key=len, reverse=True)[:int(math.log(len(descs), 2))], sep=\"\\n\")\n",
    "#         print()\n",
    "\n",
    "extra_stopwords = {\n",
    "    \"character\", \"characters\", \"writer\", \"author\", \"one\", \"first\", \"second\", \"last\"\n",
    "    \"whole\", \"book\", \"books\", \"series\", \"man\", \"woman\", \"place\", \"next\",\n",
    "    \"able\", \"read\", \"reader\", \"main\", \"many\"\n",
    "}\n",
    "\n",
    "print_descriptions(character_descriptions, min_descs=10, extra_stopwords=extra_stopwords)\n",
    "print_descriptions(aspect_descriptions, min_descs=10, extra_stopwords=extra_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "forbidden_sentimentals = {\n",
    "    \"thing\", \"things\", \"one\", \"way\", \"part\", \"deal\", \"job\", \"first\",\n",
    "    \"book\", \"books\", \"serie\", \"series\", \"story\", \"stories\", \"read\",\n",
    "    \"literature\"\n",
    "}\n",
    "print_descriptions(sentimental_descriptions, min_descs=10, extra_stopwords=forbidden_sentimentals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_desc_count = sum(len(word_descs[1]) for word_descs in character_descriptions.items())\n",
    "aspect_desc_count = sum(len(word_descs[1]) for word_descs in aspect_descriptions.items())\n",
    "sentimental_desc_count = sum(len(word_descs[1]) for word_descs in sentimental_descriptions.items())\n",
    "\n",
    "print(\"characters\", character_desc_count)\n",
    "print(\"aspects\", aspect_desc_count)\n",
    "print(\"sentimental\", sentimental_desc_count)\n",
    "print(\"total\", len(list(doc.sents)))\n",
    "print()\n",
    "\n",
    "sum_ = character_desc_count + aspect_desc_count + sentimental_desc_count\n",
    "total = len(list(doc.sents))\n",
    "\n",
    "print(\"percentage\", f\"{sum_ / total * 100:.2f} %\")\n",
    "\n",
    "count = 0\n",
    "unique_descs = 0\n",
    "word_descs = 0\n",
    "for descss in [character_descriptions, aspect_descriptions, sentimental_descriptions]:\n",
    "    for key, descs in descss.items():\n",
    "        count = len(descs)\n",
    "        unique_descs += len(set(\" \".join(w.text for w in desc) for desc in descs))\n",
    "        word_descs += sum(len(desc) for desc in descs)\n",
    "\n",
    "print(\"count\", count)\n",
    "print(\"unique_descs\", unique_descs)\n",
    "print(\"word_descs\", word_descs)\n",
    "\n",
    "print(len(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"data/goodreads_reviews_fantasy_paranormal.json\") as from_:\n",
    "#     with open(\"data/jsonified.json\", \"w\") as to_:\n",
    "#         to_.write(\"[\\n\")\n",
    "#         for line in tqdm(from_):\n",
    "# #             review = json.loads(line)\n",
    "#             to_.write(line)\n",
    "#             to_.write(\",\\n\")\n",
    "#         to_.write(\"]\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_frame = pd.read_json(\"data/jsonified.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
